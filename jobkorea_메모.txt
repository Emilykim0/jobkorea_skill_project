[ 잡코리아(잡코리아+알바몬) 사이트에서 특정 키워드에 대한 공고 -> 각 공고에서 요구하는 스킬부분 크롤링하기 ]
# 한 페이지에 20개의 공고씩 있음 3페이지까지 총 60개 공고 분석하기
# 그 이상은 연관성이 떨어지는 공고들이 있어서 3페이지까지만 하기!!

1. 모듈 로딩
# BeautifulSoup : 웹페이지(HTML)를 분석해서 원하는 데이터를 쉽게 추출할 수 있도록 도와주는 Python 라이브러리
# Selenium : 동적 웹크롤링, 즉 웹페이지를 직접 열어서 크롤링 (버튼 클릭, 로그인, 스크롤 기능)
    -> 각 키워드에 대한 페이지에서 공고 페이지를 클릭해서 다른 페이지를 열어 크롤링할 것 임 -> Selenium 필요!
그 외 모듈 및 라이브러리는 코드마다 주석으로 설명 적어 놓음

2. 클래스 만들기 - JobKoreaSkillScraper 
# 키워드 입력되면 원하는 기능(크롤링 -> 그래프 그리기 및 워드클라우드 생성)하는 클래스 만들기
# 여러 키워드(["빅데이터", "인공지능", "Python", "SQL", "머신러닝", "딥러닝", "자연어처리"])에 대해 크롤링 할 것임
-> 키워드만 바꿔서 크롤링 기능을 재사용할 수 있도록 클래스 만들기!

2-1. 생성자: __init__()
# self.base_url -> 잡코리아 검색 URL 기본 주소
# self.keywords -> 검색할 키워드 리스트 (예: "빅데이터", "AI")
# self.max_pages -> 각 키워드별 최대 몇 개 페이지를 크롤링할지 설정
# self.max_jobs -> 각 페이지에서 몇 개의 공고를 크롤링할지 설정
# self.driver -> Chrome 브라우저 실행
# self.all_skills -> 각 키워드별로 크롤링한 스킬을 저장하는 딕셔너리 {keyword1: [스킬1,스킬2,...],keyword2: [스킬1,스킬2,...]}
# self.skill_data -> 각 키워드별 데이터프레임을 저장할 리스트

2-2. get_job_links() 함수
특정 키워드에 따른 잡코리아 검색 결과 사이트에서 채용공고 링크를 가져오는 함수
# 잡코리아 검색 결과 사이트 URL은 다음과 같은 패턴을 가짐
: https://www.jobkorea.co.kr/Search/?stext={특정키워드}&tabType=recruit&Page_No={페이지넘버}
# a 태그에 class가 information-title-link.dev-view인 부분에 있는 'href='에 공고 링크가 있음
    select는 해당하는 모든 내용 가져와서 리스트로 담음 -> job_elements 리스트로 저장됨
# 각 페이지에서 최대 공고 개수(max_jobs)만큼의 공고 링크 요소가 job_elements에 담겨있음
    각 job_elements에서 href="공고링크" 부분에서 공고링크 추출하기(반복문으로)
    -> job_link라는 빈 리스트에 저장하기(함수내부에 빈리스트 미리 만들어놓기)

2-3. scrape_skill_tags()
# 특정 채용 공고 페이지에 들어가서 '스킬'항목 크롤링하기
# 보통 <dl> 태그의 class tbList의 <dd>부분에 스킬 항목이 있음, <dt>에는 '스킬'이라고 적혀져 있음
 * 아닌 경우가 있음!!ㅜㅜ -> 'div'에서 tbList 찾기
-> <dl class='tbList'> 밑에 <dt> 스킬 밑에 <dd>의 text 정보 가져와서 ',' 로 분리해서 리스트에 담기

2-4. collect_skills()
get_job_links()와 scrape_skill_tags()를 조합해서 전체 키워드별 스킬 데이터를 수집하는 함수
# (1) 각 키워드별로 채용공고 링크를 가져오고
# (2) 개별 공고에서 스킬데이터를 크롤링한 후,
# (3) DataFrame으로 변환하여 저장하기
# 자신의 검색 키워드는 제외하고 상위 10개 저장하기
# 

2-5. plot_top_skills()
상위 10개 스킬의 빈도수 그래프 생성 (검색 키워드는 제외)
# self.skill_data에는 여러개의 DataFrame이 저장되어 있음.
 -> dataframe을 하나씩 꺼내서 특정 데이터프레임으로 존재하도록 미리 빈 데이터프레임을 만들어두기 (df)
# self.all_skills라는 dict에서 key부분(즉, 키워드)을 하나씩 꺼내서 그래프 그리기(가장 큰(바깥쪽) 반복문)
##  DataFrame을 하나씩 꺼내왔을 때, 빈 dataframe이 아니라면(뭔가 DataFrame이 잘못 만들어졌는 경우),
    그리고 '키워드'라는 컬럼이 존재하고,
    그리고 키워드 컬럼의 요소(원소)가 특정 키워드(반복문으로 하나씩 꺼내오는 키워드)와 일치할 때,
    -> 그 dataframe을 df로 저장하기 
# 만약, df가 비어있거나(리스트에서 꺼낸 DataFrame으로 저장되지 못하고 기본값 빈 df로 존재하는 경우)
       df의 '단어'컬럼에 해당하는 부분이 비어져 있으면(저장된 스킬 단어가 없으면)
       -> 걍 다음 과정 건너뛰고 그 반복문부분으로 돌아가서 다음차례 진행(continue)
# 그래프 그리기(하늘색)

2-6. generate_wordclouds()
키워드별 워드 클라우드 생성 함수
# cloud.png 모양으로 만들기
# 빈도수 상위 20개 단어로만 만들기


# 아.. 60~100정도 사이쯤으로 공고링크 접근하면 잡코리아에서 막음!!
수동으로 인증해서 풀어줘야함!!!!! # 잘 지켜봐주기..

